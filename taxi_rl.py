# -*- coding: utf-8 -*-
"""Taxi_RL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NsWOKAQJ7s8ST0IMr4dUbK1gAFFtYHAB
"""

!pip install gym
!pip install matplotlib
!pip install numpy







import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import gym
import time
import os
from matplotlib.animation import FuncAnimation, FFMpegWriter

# Initialize environment
env = gym.make("Taxi-v3")

# Hyperparameter ranges
learning_rates = [0.1, 0.3, 0.5, 0.7, 0.9]
decay_rates = [0.95, 0.99, 0.999]
num_episodes_list = [1000, 2000, 3000, 4000, 5000]

# Store results
results = []

# Set up the video writer (optional: adjust the file path as needed)
writer = FFMpegWriter(fps=10)  # fps is the speed of the video

# Loop through each hyperparameter combination
for learning_rate in learning_rates:
    for decay_rate in decay_rates:
        for num_episodes in num_episodes_list:
            # Initialize Q-table
            state_space_size = env.observation_space.n
            action_space_size = env.action_space.n
            q_table = np.zeros((state_space_size, action_space_size))

            # Other parameters
            gamma = 0.6  # Discount factor
            epsilon = 1.0  # Start with full exploration
            min_epsilon = 0.01  # Minimum exploration rate

            # Lists to store rewards for training and validation
            training_rewards = []
            validation_rewards = []

            # Training loop
            for episode in range(num_episodes):
                state = env.reset()
                total_reward = 0
                done = False

                while not done:
                    # Choose an action based on epsilon-greedy policy
                    if np.random.rand() < epsilon:
                        action = env.action_space.sample()  # Explore
                    else:
                        action = np.argmax(q_table[state])  # Exploit

                    # Take action, observe result
                    next_state, reward, done, _ = env.step(action)
                    total_reward += reward

                    # Update Q-value
                    best_next_action = np.argmax(q_table[next_state])
                    q_table[state, action] += learning_rate * (
                        reward + gamma * q_table[next_state, best_next_action] - q_table[state, action]
                    )

                    # Update state
                    state = next_state

                # Update epsilon for decay
                epsilon = max(min_epsilon, epsilon * decay_rate)
                training_rewards.append(total_reward)


            validation_total_rewards = []
            validation_episodes = 100
            rewards = []

            for _ in range(validation_episodes):
                state = env.reset()
                done = False
                total_reward = 0
                while not done:
                    action = np.argmax(q_table[state])  # Exploit only (no exploration)
                    next_state, reward, done, _ = env.step(action)
                    total_reward += reward
                    state = next_state

                validation_total_rewards.append(total_reward)
                rewards.append(total_reward)


            average_validation_reward = np.mean(validation_total_rewards)
            validation_rewards.append(average_validation_reward)

            # Saving results
            results.append((learning_rate, decay_rate, num_episodes, average_validation_reward))
            print(f"Learning Rate: {learning_rate}, Decay Rate: {decay_rate}, Episodes: {num_episodes}, Average Validation Reward: {average_validation_reward}")

            #
            frames = []
            for episode in range(10):  #val episode is only 10
                state = env.reset()
                done = False
                total_rewards = 0
                for step in range(50):  # Max steps in an episode
                    action = np.argmax(q_table[state])  # Best action
                    new_state, reward, done, info = env.step(action)
                    total_rewards += reward


                    frame = env.render(mode="rgb_array")  # stores the  grid in RGB image format
                    frames.append(frame)

                    if done:
                        break
                    state = new_state

            # Create the animation
            fig, ax = plt.subplots(figsize=(5, 5))

            def update_frame(frame):
                ax.clear()  # Clear the previous frame
                ax.imshow(frame)  # Show the RGB image frame
                ax.axis('off')


            plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05)

            ani = FuncAnimation(fig, update_frame, frames=frames, interval=200, repeat=False)

            # Save the animation as a video file named after hyperparameters
            movie_filename = f'taxi_RL_training_movie_lr{learning_rate}_dr{decay_rate}_ep{num_episodes}.mp4'
            ani.save(movie_filename, writer=writer)

# Convert results to DataFrame
results_df = pd.DataFrame(results, columns=["Learning Rate", "Decay Rate", "Episodes", "Average Validation Reward"])

# Save results to CSV and NumPy files
results_df.to_csv("taxi_RL_results.csv", index=False)
np.save("taxi_RL_results.npy", np.array(results))

# Plotting the results
plt.figure(figsize=(12, 6))
for decay_rate in decay_rates:
    subset = results_df[results_df["Decay Rate"] == decay_rate]
    plt.plot(subset["Episodes"], subset["Average Validation Reward"], marker='o', label=f'Decay Rate: {decay_rate}')

plt.title('Average Validation Reward vs. Number of Episodes')
plt.xlabel('Number of Episodes')
plt.ylabel('Average Validation Reward')
plt.legend()
plt.grid()
plt.savefig("taxi_RL_results_plot.png")  #  plot into a PNG file
plt.show()